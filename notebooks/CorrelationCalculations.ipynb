{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected correlations/uncertainties\n",
    "It is worth calculation expected correlations - both as they are interesting in themselves, and because I (thought I) need(ed) to calculate the analytical Hessian anyway, to improve high stats toys, where numerical estimates of the Hessian run into trouble). (This was resolved by turning to `autograd` as discussed below).\n",
    "\n",
    "We minimize the log likelihood function\n",
    "\n",
    "$$\n",
    "-\\log \\mathcal L \\simeq \\sum_i \\frac {(N_i - \\hat N_i(\\theta))^2}{2N_i} \n",
    "$$\n",
    "\n",
    "where $\\theta$ are the parameters we try to estimate (which, depending on setup can be $\\{x_\\pm, y_\\pm\\}$ or $\\{x_\\pm, y_\\pm, F_i\\}$ (for now ignoring that we are actually using a Poissonian likelihood in the full fit). The yields are, as usual\n",
    "\n",
    "$$ \n",
    "\\hat N^+_i = h^+\\frac{Y^+_i}{\\sum_k Y^+_k}\\quad,\\quad Y_i^+=(F_{-i}+(x_+^2+y_+^2)F_i + 2\\sqrt{F_iF_{-i}}(c_i x_+-s_i y_+)\n",
    "$$\n",
    "$$\n",
    "\\hat N^-_i = h^-\\frac{Y^-_i}{\\sum_k Y^-_k}\\quad,\\quad Y_i^-=(F_{i}+(x_-^2+y_-^2)F_{-i} + 2\\sqrt{F_iF_{-i}}(c_i x_-+s_i y_-)\n",
    "$$\n",
    "\n",
    "\n",
    "the covariance matrix $V$ can be estimated from the inverse of the Hessian matrix at the minimum\n",
    "\n",
    "$$\\hat V = \\left( \\frac{\\partial^2 (-\\log \\mathcal L)}{\\partial \\hat\\theta_i\\partial\\hat\\theta_j   }\\right)^{-1} $$\n",
    "\n",
    "(where a **matrix inverse** is taken). For our specific likelihood function, we find\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial^2 (-\\log \\mathcal L)}{\\partial \\theta_i\\partial\\theta_j   }\n",
    "=\n",
    "\\sum_i\n",
    "\\frac {\\hat N_i(\\theta)-N_i}{N_i} \\frac{\\partial^2 \\hat N_i(\\theta)}{\\partial \\theta_i\\partial\\theta_j}\n",
    "+\n",
    "\\frac{1}{N_i}\n",
    "\\left(\\frac{\\partial \\hat N_i(\\theta)}{\\partial \\theta_i}\\right)\n",
    "\\left(\\frac{\\partial \\hat N_i(\\theta)}{\\partial \\theta_j}\\right)\n",
    "$$\n",
    "\n",
    "We can simplify this, by noting that we are looking for the infinite statistics case, in which case the first term in negligible, as it scales as $\\sqrt{N}$ whereas the latter term scales as $N$. Furthermore,  on average $N_i=\\hat N_i(\\theta)$, so we are looking for\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\hat V = \\left( \n",
    "\\sum_i\n",
    "\\frac{1}{\\hat N_i(\\theta)}\n",
    "\\left(\\frac{\\partial \\hat N_i(\\theta)}{\\partial \\theta_i}\\right)\n",
    "\\left(\\frac{\\partial \\hat N_i(\\theta)}{\\partial \\theta_j}\\right)\n",
    "\\right)^{-1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that\n",
    "$$\n",
    "\\frac{\\partial N_i^\\pm}{\\partial \\theta_j} = \\frac{h^\\pm}{\\sum_k Y^\\pm_k}\\left(\\frac{\\partial Y^\\pm_i}{\\partial \\theta_j}-\\frac{Y^\\pm_i}{\\sum_\\ell Y^\\pm_\\ell}\\sum_m\\frac{\\partial Y^\\pm_m}{\\partial \\theta_j}\\right).\n",
    "$$\n",
    "For the CP-parameters:\n",
    "$$\n",
    "\\frac{\\partial Y^\\pm_i}{\\partial x_\\pm} = 2 h^\\pm x_\\pm F_{\\pm i} + 2h^\\pm \\sqrt{F_i F_{-i}}c_i\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial Y^\\pm_i}{\\partial y_\\pm} = 2 h^\\pm y_\\pm F_{\\pm i} \\mp 2h^\\pm \\sqrt{F_i F_{-i}}s_i\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial Y^\\mp_i}{\\partial x_\\pm} = \\frac{\\partial Y^\\mp_i}{\\partial y_\\pm} = 0.\n",
    "$$\n",
    "For the $F_i$ parameters\n",
    "$$\n",
    "\\frac{\\partial Y^+_i }{\\partial F_i} = h^+\\left(x_+^2 + y_+^2 + \\frac{F_{-i}(c_i x_+ - s_i y_+)}{\\sqrt{F_i F_{-i}}}\\right)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial Y^+_{-i} }{\\partial F_{i}} = h^+\\left(1 + \\frac{F_{-i}(c_{-i} x_+ - s_{-i} y_+)}{\\sqrt{F_i F_{-i}}}\\right)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial Y^-_{-i} }{\\partial F_{i}} = h^-\\left(x_-^2 + y_-^2 + \\frac{F_{-i}(c_{-i} x_- + s_{-i} y_-)}{\\sqrt{F_i F_{-i}}}\\right)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial Y^-_i }{\\partial F_{i}} = h^-\\left(1 + \\frac{F_{-i}(c_i x_- + s_i y_-)}{\\sqrt{F_i F_{-i}}}\\right)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial Y^\\pm_i}{\\partial F_j} = 0 \\text{     for } |j|\\neq |i|\n",
    "$$\n",
    "In these calculations, we will pretend the fit is made with the final $F_N$ fixed, which removes the arbitrary normalisation in the equations (if this is not done, the obtained Hession cannot be inverted). \n",
    "\n",
    "For us in the calculations below, we use a DefaultModel.py and load $F_i, c_i, s_i$ corresponding to either KsPiPi or KsKK. Here the physics paramters are also entered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using defualt (trivial) efficiency\n"
     ]
    }
   ],
   "source": [
    "# Use the KS framework for calculations\n",
    "from models import DefaultModel\n",
    "import UtilityFunctions as  uf\n",
    "import UsefulInputs as ui\n",
    "\n",
    "dm = DefaultModel(\"../../amplitude_calculations/output/KS_default.pickle\",\n",
    "                 binning_file=\"../../python/input/KsPiPi_optimal.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use Fi, ci, si also used in toys to compare results\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "use_KsKK = 0\n",
    "if use_KsKK:\n",
    "    dm.set_Fi(ui.run2_Fi_DD_KsKK) # from the LHCb measurement input\n",
    "    dm.set_ci_si(ui.cleo_ci_KsKK, ui.cleo_si_KsKK)\n",
    "else:\n",
    "    dm.set_Fi(ui.run2_Fi_DD) # from the LHCb measurement input\n",
    "    dm.set_ci_si(ui.cleo_ci, ui.cleo_si)\n",
    "    \n",
    "physics_param = [uf.deg_to_rad(75.), 0.1, uf.deg_to_rad(140.)]\n",
    "xy = uf.get_xy(physics_param) # calculate x, y, N for these physics param\n",
    "\n",
    "Nplus = 1000\n",
    "Nminus = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python definitions\n",
    "This section just defines the functions above in python and can be skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_yields(xyF):\n",
    "    xy = xyF[0:4]\n",
    "    F = xyF[4:]\n",
    "    old_F = dm.Fi\n",
    "    dm.set_Fi(F)\n",
    "    yields = dm.predict_yields(xy, Nplus, Nminus)\n",
    "    dm.set_Fi(old_F)\n",
    "    return yields\n",
    "\n",
    "def calc_Y(xyF):\n",
    "    xy = xyF[0:4]\n",
    "    F = xyF[4:]\n",
    "    old_F = dm.Fi\n",
    "    dm.set_Fi(F)\n",
    "    Y = dm.predict_yields(xy, 1, 1, normalize=False)\n",
    "    dm.set_Fi(old_F)\n",
    "    return Y\n",
    "\n",
    "def calc_chi2(xyF, data):\n",
    "    xy = xyF[0:4]\n",
    "    F = xyF[4:]\n",
    "    old_F = dm.Fi\n",
    "    dm.set_Fi(F)\n",
    "    chi2 = dm.yields_chi_square(xy, data)\n",
    "    dm.set_Fi(old_F)\n",
    "    return chi2\n",
    "\n",
    "def num_grad_Y(xyF, i, delta=1e-5):\n",
    "    delta_xyF = 0 * xyF\n",
    "    delta_xyF[i] = delta\n",
    "    y1 = calc_Y(xyF+delta_xyF)\n",
    "    y2 = calc_Y(xyF-delta_xyF)\n",
    "    return (y1 - y2)/(2*delta)\n",
    "\n",
    "def ana_grad_Y(xyF, i):\n",
    "    xy = xyF[0:4]\n",
    "    F = xyF[4:]\n",
    "    F_inv = F[-1::-1]\n",
    "    Ni = calc_yields(xyF) # just to get bin number ...\n",
    "    N=len(Ni)/2\n",
    "    grad = 0*Ni\n",
    "    if i == 0: # xm\n",
    "        grad[N:] = 2*xy[i]*F_inv+2*np.sqrt(F*F_inv)*dm.ci\n",
    "    if i == 1: # ym\n",
    "        grad[N:] = 2*xy[i]*F_inv+2*np.sqrt(F*F_inv)*dm.si\n",
    "    if i == 2: # xp\n",
    "        grad[0:N] = 2*xy[i]*F+2*np.sqrt(F*F_inv)*dm.ci\n",
    "    if i == 3: # yp\n",
    "        grad[0:N] = 2*xy[i]*F-2*np.sqrt(F*F_inv)*dm.si\n",
    "    if i >= 4: # one of the Fi\n",
    "        bin_i = i - 4\n",
    "        bin_i_inv = N-1-bin_i\n",
    "        # N+ i contribution\n",
    "        grad[bin_i] = xy[2]**2+xy[3]**2+F_inv[bin_i]*(dm.ci[bin_i]*xy[2]-dm.si[bin_i]*xy[3])/np.sqrt(F[bin_i]*F_inv[bin_i])\n",
    "        # N- i contribution\n",
    "        grad[bin_i+N] = 1+F_inv[bin_i]*(dm.ci[bin_i]*xy[0]+dm.si[bin_i]*xy[1])/np.sqrt(F[bin_i]*F_inv[bin_i])\n",
    "        # N+ -i contribution\n",
    "        grad[bin_i_inv] = 1+F_inv[bin_i]*(dm.ci[bin_i_inv]*xy[2]-dm.si[bin_i_inv]*xy[3])/np.sqrt(F[bin_i]*F_inv[bin_i])\n",
    "        # N- -i contribution\n",
    "        grad[bin_i_inv+N] = xy[0]**2+xy[1]**2+F_inv[bin_i]*(dm.ci[bin_i_inv]*xy[0]+dm.si[bin_i_inv]*xy[1])/np.sqrt(F[bin_i]*F_inv[bin_i])\n",
    "    return grad\n",
    "\n",
    "def comp_grad_Y(i):\n",
    "    print \"N_gY_{}\".format(i), num_grad_Y(np.concatenate([xy, dm.Fi]), i)\n",
    "    print \"A_gY_{}\".format(i), ana_grad_Y(np.concatenate([xy, dm.Fi]), i)\n",
    "    print \"-\"\n",
    "\n",
    "test = False\n",
    "if test:\n",
    "    yields = dm.predict_yields(xy, 1000, 1000)\n",
    "    for i in range(4+len(yields)/2):\n",
    "        comp_grad_Y(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def num_grad_yields(xyF, i, delta=1e-5):\n",
    "    delta_xyF = 0 * xyF\n",
    "    delta_xyF[i] = delta\n",
    "    y1 = calc_yields(xyF+delta_xyF)\n",
    "    y2 = calc_yields(xyF-delta_xyF)\n",
    "    return (y1 - y2)/(2*delta)\n",
    "\n",
    "def ana_grad_yields(xyF, i):\n",
    "    Ni = calc_yields(xyF) # just to get bin number ...\n",
    "    N=len(Ni)/2\n",
    "    hp = sum(Ni[:N])\n",
    "    hm = sum(Ni[N:])\n",
    "    Y = calc_Y(xyF)\n",
    "    dY = ana_grad_Y(xyF, i)\n",
    "    Yp = Y[:N]; \n",
    "    Ym = Y[N:]\n",
    "    dYp = dY[:N]; \n",
    "    dYm = dY[N:]\n",
    "    grad_p = hp/sum(Yp)*(dYp-Yp*sum(dYp)/sum(Yp))\n",
    "    grad_m = hm/sum(Ym)*(dYm-Ym*sum(dYm)/sum(Ym))\n",
    "    return np.concatenate([grad_p, grad_m])\n",
    "\n",
    "def comp_grad_N(i):\n",
    "    print \"N_gN_{}\".format(i), num_grad_yields(np.concatenate([xy, dm.Fi]), i)\n",
    "    print \"A_gN_{}\".format(i), ana_grad_yields(np.concatenate([xy, dm.Fi]), i)\n",
    "    print \"-\"\n",
    "test = False\n",
    "if test:\n",
    "    for i in range(4+len(yields)/2):\n",
    "        comp_grad_N(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def num_grad_chi2(xyF, i, data, delta=1e-5):\n",
    "    delta_xyF = 0 * xyF\n",
    "    delta_xyF[i] = delta\n",
    "    y1 = calc_chi2(xyF+delta_xyF, data)\n",
    "    y2 = calc_chi2(xyF-delta_xyF, data)\n",
    "    return (y1 - y2)/(2*delta)\n",
    "\n",
    "def ana_grad_chi2(xyF, i, data):\n",
    "    Nhat = calc_yields(xyF)\n",
    "    dNhat = ana_grad_yields(xyF, i)\n",
    "    grad_chi2 = sum( (Nhat-data)/data*dNhat)\n",
    "    return grad_chi2\n",
    "\n",
    "def comp_grad_chi2(xyF, i):\n",
    "    print \"N_gChi2_{}\".format(i), num_grad_chi2(xyF, i, yields)\n",
    "    print \"A_gChi2_{}\".format(i), ana_grad_chi2(xyF, i, yields)\n",
    "    print \"-\"\n",
    "\n",
    "test = False\n",
    "if test:\n",
    "    for i in range(4+len(yields)/2):\n",
    "        print \"Exact sol\"\n",
    "        comp_grad_chi2(np.concatenate([xy, dm.Fi]), i)\n",
    "        print \"Crappy Fi\"\n",
    "    #     comp_grad_chi2(np.concatenate([xy, [0.25, 0.25, 0.25, 0.25]]), i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def num_hess_yields(xyF, i, j, delta=1e-5):\n",
    "    delta_xyF = 0 * xyF\n",
    "    delta_xyF[j] = delta\n",
    "    y1 = num_grad_yields(xyF+delta_xyF, i)\n",
    "    y2 = num_grad_yields(xyF-delta_xyF, i)\n",
    "    return (y1 - y2)/(2*delta)\n",
    "\n",
    "def num_hess_chi2(xyF, i, j, data, delta=1e-5):\n",
    "    delta_xyF = 0 * xyF\n",
    "    delta_xyF[j] = delta\n",
    "    y1 = num_grad_chi2(xyF+delta_xyF, i, data)\n",
    "    y2 = num_grad_chi2(xyF-delta_xyF, i, data)\n",
    "    return (y1 - y2)/(2*delta)\n",
    "\n",
    "def ana_simple_hess_chi2(xyF, i, j): # Does not take data, as it is a proximation close to an optimal solution\n",
    "    N = calc_yields(xyF)\n",
    "    dN_di = ana_grad_yields(xyF, i)\n",
    "    dN_dj = ana_grad_yields(xyF, j)\n",
    "    return sum(dN_di*dN_dj/N)\n",
    "\n",
    "def comp_hess_chi2(xyF, i, j, data):\n",
    "    print \"N_hess_Chi2_{}_{}       \".format(i, j), num_hess_chi2(xyF, i, j, data)\n",
    "    print \"A_simple_hess_Chi2_{}_{}\".format(i, j), ana_simple_hess_chi2(xyF, i, j)\n",
    "    print \"-\"\n",
    "    \n",
    "test = False\n",
    "if test:\n",
    "    for i in range(4+len(yields)/2):\n",
    "        for j in range(4+len(yields)/2):\n",
    "            comp_hess_chi2(np.concatenate([xy, dm.Fi]), i, j, yields)\n",
    "    #         comp_hess_chi2(np.concatenate([xy, [0.25, 0.25, 0.25, 0.25]]), i, j, yields)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a few utility functions \n",
    "def get_V(H):\n",
    "    return np.linalg.inv(H)\n",
    "\n",
    "def get_uncertainties(H):\n",
    "    V = get_V(H)\n",
    "    return np.sqrt(np.diag(V))\n",
    "\n",
    "def get_correlations(H):\n",
    "    unc = get_uncertainties(H)\n",
    "    V = get_V(H)\n",
    "    diag_mat = np.diag(1/unc)\n",
    "    corr = diag_mat.dot(V).dot(np.transpose(diag_mat))\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the calculation results\n",
    "We can now calculate the Hession matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start constructing the Hessian\n",
    "H = np.zeros((4+len(dm.Fi), 4+len(dm.Fi))) # 4 xy, 16 Fi's\n",
    "H_num = H\n",
    "for i in range(4+len(dm.Fi)):\n",
    "    for j in range(4+len(dm.Fi)):\n",
    "        H[i, j] = ana_simple_hess_chi2(np.concatenate([xy, dm.Fi]), i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple case: only floating x and y\n",
    "When only floating $x$ and $y$ parameters, the hessian matrix is a 2x2 2x2 block matrix, as the equations decouple completely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1673.465  363.372    0.       0.   ]\n",
      " [ 363.372 1478.303    0.       0.   ]\n",
      " [   0.       0.    2139.898 -331.005]\n",
      " [   0.       0.    -331.005 3003.441]]\n"
     ]
    }
   ],
   "source": [
    "Hxy = H[0:4,0:4]\n",
    "print Hxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix can be inverted to get the covariance matrix, from which the expected uncertainties and correlations can be obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainties (xm, ym, xp, yp)\n",
      "[0.025 0.027 0.022 0.018]\n",
      "\n",
      "Correlations:\n",
      "[[ 1.    -0.231  0.     0.   ]\n",
      " [-0.231  1.     0.     0.   ]\n",
      " [ 0.     0.     1.     0.131]\n",
      " [ 0.     0.     0.131  1.   ]]\n"
     ]
    }
   ],
   "source": [
    "print \"Uncertainties (xm, ym, xp, yp)\"\n",
    "print get_uncertainties(Hxy)\n",
    "print \"\\nCorrelations:\"\n",
    "print get_correlations(Hxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can for example be compare to a high stats signal only toy study with fixed Ki, one specific relisation of which gave\n",
    "\n",
    "$$\\rho(x_-, y_-) = -0.217 \\qquad \\rho(x_+, y_+)=+0.167$$\n",
    "\n",
    "which certainly agrees very well with above (input $rB=0.1$, $\\gamma=75^\\circ$, $\\delta_B=140^\\circ$ above is necessary for the two numbers to match ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens if the $F_i$ are freed?\n",
    "In that case, we include the partial derivatives of all but the last $F_i$ in the Hessian that we invert (corresponsing to the last $F_i$ being fixed, to remove an arbitrary normalisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainties (xm, ym, xp, yp)\n",
      "[0.091 0.107 0.091 0.082]\n",
      "\n",
      "Correlations:\n",
      "[[1.    0.487 0.922 0.633]\n",
      " [0.487 1.    0.601 0.928]\n",
      " [0.922 0.601 1.    0.725]\n",
      " [0.633 0.928 0.725 1.   ]]\n"
     ]
    }
   ],
   "source": [
    "HF = H[0:-1,0:-1]\n",
    "print \"Uncertainties (xm, ym, xp, yp)\"\n",
    "print get_uncertainties(HF)[0:4]\n",
    "print \"\\nCorrelations:\"\n",
    "print get_correlations(HF)[0:4,0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the uncertainties on x and y increased by a factor 4! Furthermore, very large correlations are now exhibited between $x_+$ and $x_-$ (same for $y$), due to the equations beeing heavily coupled by the unconstrained $F_i$. \n",
    "\n",
    "This behaviour is even worse for small $r_B$, as can be seen by changing the input value above.\n",
    "\n",
    "Below, we print correlations in the same order as the toy setups, for easy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000 0.922 0.487 0.633\n",
      "      1.000 0.601 0.725\n",
      "            1.000 0.928\n",
      "                  1.000 \n"
     ]
    }
   ],
   "source": [
    "corr = get_correlations(HF)[0:4,0:4]\n",
    "\n",
    "print \"{:1.3f} {:1.3f} {:1.3f} {:1.3f}\".format(corr[0,0], corr[0,2], corr[0,1], corr[0,3])\n",
    "print \" \"*6+\"{:1.3f} {:1.3f} {:1.3f}\".format(corr[2,2],  corr[2,1], corr[2,3])\n",
    "print \" \"*12+\"{:1.3f} {:1.3f}\".format(  corr[1,1], corr[1,3])\n",
    "print \" \"*18+\"{:1.3f} \".format(corr[3,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding constraints to the $F_i$\n",
    "One can use this framework to examine the effect of constraints on $F_i$ by extending the likelihood function and adding a term to the Hessian elements involving $F_i$. I haven't done so, however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A test of autograd versus minimize Hessians\n",
    "We can use these calculations to compare\n",
    "* the analytical Hessian\n",
    "* the Hessian returned by the dm.fit call (res.hess_inv)\n",
    "* the Hessian as calculated by `autograd`, an automatical differentiation library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1673.465  363.372    0.       0.   ]\n",
      " [ 363.372 1478.303    0.       0.   ]\n",
      " [   0.       0.    2139.898 -331.005]\n",
      " [   0.       0.    -331.005 3003.441]]\n"
     ]
    }
   ],
   "source": [
    "H1 = Hxy\n",
    "print H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting data using DefaultModel()\n",
      " Succes: True\n",
      " Msg   : Optimization terminated successfully.\n",
      " Fun   : 2.58502599518e-13\n",
      " p-val : 1.0\n",
      "[[1645.209  355.038  -14.616   11.847]\n",
      " [ 355.038 1436.128  -21.162   20.198]\n",
      " [ -14.616  -21.162 2110.308 -303.701]\n",
      " [  11.847   20.198 -303.701 2978.665]]\n"
     ]
    }
   ],
   "source": [
    "n = dm.predict_yields(xy, Nplus, Nminus)\n",
    "res = dm.fit(n)\n",
    "H2 = np.linalg.inv(res.hess_inv)\n",
    "print(H2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1673.465  363.372    0.       0.   ]\n",
      " [ 363.372 1478.302    0.       0.   ]\n",
      " [   0.       0.    2139.898 -331.006]\n",
      " [   0.       0.    -331.006 3003.441]]\n"
     ]
    }
   ],
   "source": [
    "from autograd import hessian\n",
    "H = hessian(dm.yields_chi_square)\n",
    "H3 = H(res.x, n)\n",
    "print H3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, `autograd` indeed recovers the **exact** Hessian, without the numerical effects at play in the `minimize` version in `dm.fit()`. Therefore, now every fit result has the Hessian, covariance, and correlation matrix attached to it using autograd, before being returned in `DefaultModel::fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
